{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1cb963",
   "metadata": {},
   "source": [
    "# Production and Maintenance Planning (POMDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36296634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Grid Dimensions\n",
    "rows, cols = 3, 4  # Dimensions of the grid (3 rows, 4 columns)\n",
    "num_states = rows * cols  # Total number of states\n",
    "\n",
    "# Actions and Observations\n",
    "actions = ['continue to manufacture', 'shut down and do maintenance']\n",
    "observations = ['o1', 'o2', 'o3']\n",
    "\n",
    "# Transition Probabilities\n",
    "transition_probabilities = {\n",
    "    0: {0: [(0, 0.5), (1, 0.4), (2, 0.1)], 1: [(0, 1.0)]},\n",
    "    1: {0: [(1, 0.5), (2, 0.4), (3, 0.1)], 1: [(0, 0.9), (1, 0.1)]},\n",
    "    2: {0: [(2, 0.5), (3, 0.5)], 1: [(0, 0.6), (1, 0.3), (2, 0.1)]},\n",
    "    3: {0: [(3, 1.0)], 1: [(0, 0.3), (1, 0.3), (2, 0.3), (3, 0.1)]},\n",
    "    4: {0: [(1, 0.25), (2, 0.25), (3, 0.25), (4, 0.25)], 1: [(4, 1.0)]},\n",
    "    5: {0: [(2, 0.6), (3, 0.2), (5, 0.2)], 1: [(4, 0.9), (5, 0.1)]},\n",
    "    6: {0: [(3, 0.6), (6, 0.2), (7, 0.2)], 1: [(4, 0.6), (5, 0.3), (6, 0.1)]},\n",
    "    7: {0: [(3, 0.3), (7, 0.7)], 1: [(4, 0.3), (5, 0.3), (6, 0.3), (7, 0.1)]},\n",
    "    8: {0: [(3, 0.25), (5, 0.25), (6, 0.25), (8, 0.25)], 1: [(8, 1.0)]},\n",
    "    9: {0: [(3, 0.25), (6, 0.25), (7, 0.25), (9, 0.25)], 1: [(8, 0.9), (9, 0.1)]},\n",
    "    10: {0: [(7, 0.4), (10, 0.3), (11, 0.3)], 1: [(8, 0.6), (9, 0.3), (10, 0.1)]},\n",
    "    11: {0: [(11, 1.0)], 1: [(8, 0.3), (9, 0.3), (10, 0.3), (11, 0.1)]}\n",
    "}\n",
    "\n",
    "# Rewards: R(s, a, s') = reward for transitioning from s -> s' with action a\n",
    "reward_action = {\n",
    "    (0, 0, 0): -40, (0, 0, 1): -45, (0, 0, 2): -50, \n",
    "    (0, 1, 0): -40,\n",
    "    (1, 0, 1): -40, (1, 0, 2): -45, (1, 0, 3): -50, \n",
    "    (1, 1, 0): -70, (1, 1, 1): -40,\n",
    "    (2, 0, 2): -40, (2, 0, 3): -45, \n",
    "    (2, 1, 0): -70, (2, 1, 1): -70, (2, 1, 2): -40,\n",
    "    (3, 0, 3): -40, \n",
    "    (3, 1, 0): -90, (3, 1, 1): -70, (3, 1, 2): -70, (3, 1, 3): -40,\n",
    "    (4, 0, 1): -80, (4, 0, 2): -85, (4, 0, 3): -90, (4, 0, 4): -90,\n",
    "    (4, 1, 4): -80,\n",
    "    (5, 0, 2): -80, (5, 0, 3): -85, (5, 0, 5): -90,\n",
    "    (5, 1, 4): -110, (5, 1, 5): -80,\n",
    "    (6, 0, 3): -90, (6, 0, 6): -90, (6, 0, 7): -110,\n",
    "    (6, 1, 4): -110, (6, 1, 5): -110, (6, 1, 6): -80,\n",
    "    (7, 0, 3): -90, (7, 0, 7): -90,\n",
    "    (7, 1, 4): -130, (7, 1, 5): -110, (7, 1, 6): -110, (7, 1, 7): -80,\n",
    "    (8, 0, 3): -110, (8, 0, 5): -80, (8, 0, 6): -110, (8, 0, 8): -110,\n",
    "    (8, 1, 8): -110,\n",
    "    (9, 0, 3): -110, (9, 0, 6): -80, (9, 0, 7): -110, (9, 0, 9): -110,\n",
    "    (9, 1, 8): -140, (9, 1, 9): -110,\n",
    "    (10, 0, 7): -110, (10, 0, 10): -110, (10, 0, 11): -140,\n",
    "    (10, 1, 8): -140, (10, 1, 9): -140, (10, 1, 10): -110,\n",
    "    (11, 0, 11): -110,\n",
    "    (11, 1, 8): -160, (11, 1, 9): -140, (11, 1, 10): -140, (11, 1, 11): -110\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be300b86",
   "metadata": {},
   "source": [
    "# QMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80deabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Belief for All States:\n",
      "State 0: 0.0833\n",
      "State 1: 0.0833\n",
      "State 2: 0.0833\n",
      "State 3: 0.0833\n",
      "State 4: 0.0833\n",
      "State 5: 0.0833\n",
      "State 6: 0.0833\n",
      "State 7: 0.0833\n",
      "State 8: 0.0833\n",
      "State 9: 0.0833\n",
      "State 10: 0.0833\n",
      "State 11: 0.0833\n",
      "\n",
      "Final Alpha Vectors (One Per Action):\n",
      "Action 'continue to manufacture': [-406.44380165 -408.42975207 -404.54545455 -400.         -463.44441482\n",
      " -455.43237251 -495.51746869 -535.13513514 -524.46930873 -547.60882046\n",
      " -658.51749869 -704.40419528]\n",
      "Action 'shut down and do maintenance': [-400.         -427.75867769 -429.68512397 -436.50330579 -497.09997334\n",
      " -523.37888953 -524.82329676 -543.04861129 -582.02237786 -611.10493392\n",
      " -624.25033873 -660.44910587]\n",
      "\n",
      "Optimal Action Based on Initial Belief:\n",
      "Best action: continue to manufacture\n",
      "Action values: [-500.32901854876224, -521.6770528942701]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Grid dimensions\n",
    "rows, cols = 3, 4  # 3 rows and 4 columns\n",
    "num_states = rows * cols  # Total number of states\n",
    "num_actions = len(actions)  # Number of actions\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Initial belief (Uniform distribution)\n",
    "initial_belief = np.ones(num_states) / num_states  # Uniform belief\n",
    "\n",
    "# Initialize alpha vectors (one for each action)\n",
    "alpha_vectors = np.zeros((num_actions, num_states))\n",
    "\n",
    "# Function to update alpha vectors\n",
    "def update_alpha(alpha_vectors, transition_probabilities, reward_action, gamma):\n",
    "    new_alpha_vectors = np.zeros_like(alpha_vectors)\n",
    "    for a in range(num_actions):  # Iterate over actions\n",
    "        for s in range(num_states):  # Iterate over states\n",
    "            # Immediate reward for (s, a)\n",
    "            immediate_reward = sum(\n",
    "                prob * reward_action.get((s, a, s_prime), 0)\n",
    "                for s_prime, prob in transition_probabilities.get(s, {}).get(a, [])\n",
    "            )\n",
    "            # Future reward based on max of previous alpha vectors\n",
    "            future_reward = 0\n",
    "            for s_prime, prob in transition_probabilities.get(s, {}).get(a, []):\n",
    "                max_alpha = max(alpha_vectors[:, s_prime])\n",
    "                future_reward += prob * max_alpha\n",
    "            \n",
    "            # Update alpha value for action a at state s\n",
    "            new_alpha_vectors[a, s] = immediate_reward + gamma * future_reward\n",
    "    return new_alpha_vectors\n",
    "\n",
    "# Compute expected value of an action given the belief\n",
    "def compute_action_value(alpha_vectors, belief):\n",
    "    action_values = []\n",
    "    for a in range(num_actions):\n",
    "        value = sum(belief[s] * alpha_vectors[a, s] for s in range(num_states))\n",
    "        action_values.append(value)\n",
    "    return action_values\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 1000\n",
    "\n",
    "# Print the initial belief\n",
    "print(\"Initial Belief for All States:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"State {state}: {initial_belief[state]:.4f}\")\n",
    "\n",
    "# Iteratively update alpha vectors\n",
    "for _ in range(num_iterations):\n",
    "    alpha_vectors = update_alpha(alpha_vectors, transition_probabilities, reward_action, gamma)\n",
    "\n",
    "# Print the final alpha vectors\n",
    "print(\"\\nFinal Alpha Vectors (One Per Action):\")\n",
    "for a in range(num_actions):\n",
    "    print(f\"Action '{actions[a]}': {alpha_vectors[a]}\")\n",
    "\n",
    "# Compute the best action based on the initial belief\n",
    "action_values = compute_action_value(alpha_vectors, initial_belief)\n",
    "best_action = np.argmax(action_values)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nOptimal Action Based on Initial Belief:\")\n",
    "print(f\"Best action: {actions[best_action]}\")\n",
    "print(\"Action values:\", action_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacbde75",
   "metadata": {},
   "source": [
    "# Point-based value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60ba9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Action Based on Each Belief:\n",
      "\n",
      "Belief 0: [0.06113196 0.20967194 0.05029107 0.10603428 0.01430722 0.15209329\n",
      " 0.21423819 0.04601897 0.01569096 0.11352841 0.01172777 0.00526594]\n",
      "  Alpha vector for action 'continue to manufacture': [-364.306557   -363.73715319 -362.09964719 -358.45026496 -406.72490212\n",
      " -403.37647662 -412.91001785 -408.45026496 -422.97490212 -422.2086473\n",
      " -438.1398943  -428.45026496]\n",
      "  Value for action 'continue to manufacture': -391.1463125460281\n",
      "  Alpha vector for action 'shut down and do maintenance': [-374.73095728 -401.71995504 -401.61204228 -407.0965926  -414.73095728\n",
      " -441.71995504 -441.61204228 -447.0965926  -444.73095728 -471.71995504\n",
      " -471.61204228 -477.0965926 ]\n",
      "  Value for action 'shut down and do maintenance': -427.3777028316442\n",
      "  -> Best action: 'continue to manufacture' with value -391.1463125460281\n",
      "\n",
      "Belief 1: [0.00532849 0.25262461 0.06374673 0.15351748 0.00841892 0.09439538\n",
      " 0.16411752 0.11799836 0.00263182 0.09657412 0.02560797 0.0150386 ]\n",
      "  Alpha vector for action 'continue to manufacture': [-364.10704852 -363.95834598 -363.16388533 -360.36446994 -407.14095976\n",
      " -403.8655059  -414.48423609 -410.36446994 -423.39095976 -423.1913495\n",
      " -439.54411917 -430.36446994]\n",
      "  Value for action 'continue to manufacture': -390.06644876322105\n",
      "  Alpha vector for action 'shut down and do maintenance': [-372.82568185 -399.82056626 -399.78497007 -405.63278006 -412.82568185\n",
      " -439.82056626 -439.78497007 -445.63278006 -442.82568185 -469.82056626\n",
      " -469.78497007 -475.63278006]\n",
      "  Value for action 'shut down and do maintenance': -426.2217868666178\n",
      "  -> Best action: 'continue to manufacture' with value -390.06644876322105\n",
      "\n",
      "Belief 2: [0.03880486 0.0408444  0.13899771 0.12925002 0.28616957 0.02339195\n",
      " 0.11510349 0.01091252 0.05697905 0.0447162  0.08503917 0.02979105]\n",
      "  Alpha vector for action 'continue to manufacture': [-368.92635762 -367.9240209  -365.45160572 -360.93696183 -410.72505168\n",
      " -407.30467208 -415.74281938 -410.93696183 -426.97505168 -425.63195575\n",
      " -441.14574816 -430.93696183]\n",
      "  Value for action 'continue to manufacture': -399.9082998126815\n",
      "  Alpha vector for action 'shut down and do maintenance': [-379.13665505 -406.10637393 -405.87889266 -410.8773785  -419.13665505\n",
      " -446.10637393 -445.87889266 -450.8773785  -449.13665505 -476.10637393\n",
      " -475.87889266 -480.8773785 ]\n",
      "  Value for action 'shut down and do maintenance': -429.1187815600547\n",
      "  -> Best action: 'continue to manufacture' with value -399.9082998126815\n",
      "\n",
      "Belief 3: [0.00400767 0.01610661 0.06580415 0.07332345 0.02197715 0.13483263\n",
      " 0.16270557 0.10012623 0.09423489 0.11006708 0.15553667 0.06127789]\n",
      "  Alpha vector for action 'continue to manufacture': [-380.76414773 -379.68365794 -377.05435596 -372.38173548 -422.44837241\n",
      " -419.01441048 -427.25078367 -422.38173548 -438.69837241 -437.24995876\n",
      " -452.68530777 -442.38173548]\n",
      "  Value for action 'continue to manufacture': -424.33095573570813\n",
      "  Alpha vector for action 'shut down and do maintenance': [-389.51490051 -416.49976987 -416.41473531 -422.08609565 -429.51490051\n",
      " -456.49976987 -456.41473531 -462.08609565 -459.51490051 -486.49976987\n",
      " -486.41473531 -492.08609565]\n",
      "  Value for action 'shut down and do maintenance': -460.79808492860934\n",
      "  -> Best action: 'continue to manufacture' with value -424.33095573570813\n",
      "\n",
      "Belief 4: [0.03401556 0.08063198 0.19008935 0.00466019 0.20136814 0.02806218\n",
      " 0.18736049 0.10867221 0.04833544 0.0432858  0.01453542 0.05898324]\n",
      "  Alpha vector for action 'continue to manufacture': [-367.42443593 -367.31224637 -366.59880033 -363.87541749 -410.51509211\n",
      " -407.24672594 -417.96477062 -413.87541749 -426.76509211 -426.61617021\n",
      " -443.00944719 -433.87541749]\n",
      "  Value for action 'continue to manufacture': -402.0022570656132\n",
      "  Alpha vector for action 'shut down and do maintenance': [-379.55975986 -406.57178855 -406.73171355 -413.54691929 -419.55975986\n",
      " -446.57178855 -446.73171355 -453.54691929 -449.55975986 -476.57178855\n",
      " -476.73171355 -483.54691929]\n",
      "  Value for action 'shut down and do maintenance': -432.7508854219493\n",
      "  -> Best action: 'continue to manufacture' with value -402.0022570656132\n",
      "\n",
      "Belief 5: [0.12573033 0.13446308 0.00651075 0.03525433 0.06508604 0.090506\n",
      " 0.24263348 0.14071004 0.01041824 0.08833987 0.01971864 0.04062921]\n",
      "  Alpha vector for action 'continue to manufacture': [-366.69989221 -367.09698755 -367.38908694 -365.69031081 -410.52937755\n",
      " -407.3479045  -419.36982126 -415.69031081 -426.77937755 -427.3135268\n",
      " -444.20957649 -435.69031081]\n",
      "  Value for action 'continue to manufacture': -403.2386244857871\n",
      "  Alpha vector for action 'shut down and do maintenance': [-379.67262868 -406.69454754 -406.86178223 -413.60216413 -419.67262868\n",
      " -446.69454754 -446.86178223 -453.60216413 -449.67262868 -476.69454754\n",
      " -476.86178223 -483.60216413]\n",
      "  Value for action 'shut down and do maintenance': -435.49274980306484\n",
      "  -> Best action: 'continue to manufacture' with value -403.2386244857871\n",
      "\n",
      "Belief 6: [0.01768149 0.1507635  0.03809701 0.02062369 0.03452134 0.08365599\n",
      " 0.05726259 0.31282785 0.13600795 0.07251476 0.03035854 0.04568528]\n",
      "  Alpha vector for action 'continue to manufacture': [-373.47466721 -373.59814133 -373.34710049 -371.0970033  -416.90598016\n",
      " -413.67754404 -424.99704218 -421.0970033  -433.15598016 -433.32258203\n",
      " -449.94706161 -441.0970033 ]\n",
      "  Value for action 'continue to manufacture': -414.0177286950951\n",
      "  Alpha vector for action 'shut down and do maintenance': [-384.61583479 -411.63023401 -411.74792826 -418.28130305 -424.61583479\n",
      " -451.63023401 -451.74792826 -458.28130305 -454.61583479 -481.63023401\n",
      " -481.74792826 -488.28130305]\n",
      "  Value for action 'shut down and do maintenance': -448.53266232080273\n",
      "  -> Best action: 'continue to manufacture' with value -414.0177286950951\n",
      "\n",
      "Belief 7: [0.02792133 0.1013043  0.09429224 0.12788926 0.17613868 0.00671742\n",
      " 0.01268739 0.21774034 0.11258617 0.00090608 0.01110249 0.11071431]\n",
      "  Alpha vector for action 'continue to manufacture': [-369.50034103 -369.79052136 -369.874783   -367.96184934 -413.17610116\n",
      " -409.97669977 -421.7270228  -417.96184934 -429.42610116 -429.81747872\n",
      " -446.60960954 -437.96184934]\n",
      "  Value for action 'continue to manufacture': -403.78505784486424\n",
      "  Alpha vector for action 'shut down and do maintenance': [-379.32383041 -406.32867596 -406.38586203 -412.6708319  -419.32383041\n",
      " -446.32867596 -446.38586203 -452.6708319  -449.32383041 -476.32867596\n",
      " -476.38586203 -482.6708319 ]\n",
      "  Value for action 'shut down and do maintenance': -433.68163805104496\n",
      "  -> Best action: 'continue to manufacture' with value -403.78505784486424\n",
      "\n",
      "Belief 8: [0.03060173 0.00404383 0.04673478 0.00496925 0.07824028 0.16230691\n",
      " 0.38652861 0.06907822 0.09321997 0.04524505 0.0771421  0.00188927]\n",
      "  Alpha vector for action 'continue to manufacture': [-375.0243848  -373.59008782 -370.26059546 -364.87569818 -416.19464759\n",
      " -412.70016365 -420.02965709 -414.87569818 -432.44464759 -430.52138273\n",
      " -445.60663655 -434.87569818]\n",
      "  Value for action 'continue to manufacture': -417.65203189250474\n",
      "  Alpha vector for action 'shut down and do maintenance': [-388.78780426 -415.78510356 -415.80627166 -421.9656887  -428.78780426\n",
      " -455.78510356 -455.80627166 -461.9656887  -458.78780426 -485.78510356\n",
      " -485.80627166 -491.9656887 ]\n",
      "  Value for action 'shut down and do maintenance': -453.8810213809522\n",
      "  -> Best action: 'continue to manufacture' with value -417.65203189250474\n",
      "\n",
      "Belief 9: [0.07140852 0.00604394 0.06545138 0.32449471 0.12044414 0.00154612\n",
      " 0.0623259  0.05930062 0.01335235 0.08922571 0.04757204 0.13883458]\n",
      "  Alpha vector for action 'continue to manufacture': [-370.34799073 -369.98219681 -368.76799277 -365.53438499 -413.07072353\n",
      " -409.75908456 -419.8278281  -415.53438499 -429.32072353 -428.83165168\n",
      " -444.97454966 -435.53438499]\n",
      "  Value for action 'continue to manufacture': -398.256404725835\n",
      "  Alpha vector for action 'shut down and do maintenance': [-375.99372666 -402.96900872 -402.83324526 -408.3152797  -415.99372666\n",
      " -442.96900872 -442.83324526 -448.3152797  -445.99372666 -472.96900872\n",
      " -472.83324526 -478.3152797 ]\n",
      "  Value for action 'shut down and do maintenance': -430.1774593622968\n",
      "  -> Best action: 'continue to manufacture' with value -398.256404725835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grid Dimensions and Actions\n",
    "rows, cols = 3, 4\n",
    "num_states = rows * cols\n",
    "actions = ['continue to manufacture', 'shut down and do maintenance']\n",
    "observations = ['o1', 'o2', 'o3']\n",
    "num_actions = len(actions)\n",
    "num_observations = len(observations)\n",
    "\n",
    "# Discount Factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Transition Probabilities\n",
    "transition_probabilities = {\n",
    "    0: {0: [(0, 0.5), (1, 0.4), (2, 0.1)], 1: [(0, 1.0)]},\n",
    "    1: {0: [(1, 0.5), (2, 0.4), (3, 0.1)], 1: [(0, 0.9), (1, 0.1)]},\n",
    "    2: {0: [(2, 0.5), (3, 0.5)], 1: [(0, 0.6), (1, 0.3), (2, 0.1)]},\n",
    "    3: {0: [(3, 1.0)], 1: [(0, 0.3), (1, 0.3), (2, 0.3), (3, 0.1)]},\n",
    "    4: {0: [(1, 0.25), (2, 0.25), (3, 0.25), (4, 0.25)], 1: [(4, 1.0)]},\n",
    "    5: {0: [(2, 0.6), (3, 0.2), (5, 0.2)], 1: [(4, 0.9), (5, 0.1)]},\n",
    "    6: {0: [(3, 0.6), (6, 0.2), (7, 0.2)], 1: [(4, 0.6), (5, 0.3), (6, 0.1)]},\n",
    "    7: {0: [(3, 0.3), (7, 0.7)], 1: [(4, 0.3), (5, 0.3), (6, 0.3), (7, 0.1)]},\n",
    "    8: {0: [(3, 0.25), (5, 0.25), (6, 0.25), (8, 0.25)], 1: [(8, 1.0)]},\n",
    "    9: {0: [(3, 0.25), (6, 0.25), (7, 0.25), (9, 0.25)], 1: [(8, 0.9), (9, 0.1)]},\n",
    "    10: {0: [(7, 0.4), (10, 0.3), (11, 0.3)], 1: [(8, 0.6), (9, 0.3), (10, 0.1)]},\n",
    "    11: {0: [(11, 1.0)], 1: [(8, 0.3), (9, 0.3), (10, 0.3), (11, 0.1)]}\n",
    "}\n",
    "\n",
    "# Rewards: R(s, a, s') = reward for transitioning from s -> s' with action a\n",
    "reward_action = {\n",
    "    (0, 0, 0): -40, (0, 0, 1): -45, (0, 0, 2): -50, \n",
    "    (0, 1, 0): -40,\n",
    "    (1, 0, 1): -40, (1, 0, 2): -45, (1, 0, 3): -50, \n",
    "    (1, 1, 0): -70, (1, 1, 1): -40,\n",
    "    (2, 0, 2): -40, (2, 0, 3): -45, \n",
    "    (2, 1, 0): -70, (2, 1, 1): -70, (2, 1, 2): -40,\n",
    "    (3, 0, 3): -40, \n",
    "    (3, 1, 0): -90, (3, 1, 1): -70, (3, 1, 2): -70, (3, 1, 3): -40,\n",
    "    (4, 0, 1): -80, (4, 0, 2): -85, (4, 0, 3): -90, (4, 0, 4): -90,\n",
    "    (4, 1, 4): -80,\n",
    "    (5, 0, 2): -80, (5, 0, 3): -85, (5, 0, 5): -90,\n",
    "    (5, 1, 4): -110, (5, 1, 5): -80,\n",
    "    (6, 0, 3): -90, (6, 0, 6): -90, (6, 0, 7): -110,\n",
    "    (6, 1, 4): -110, (6, 1, 5): -110, (6, 1, 6): -80,\n",
    "    (7, 0, 3): -90, (7, 0, 7): -90,\n",
    "    (7, 1, 4): -130, (7, 1, 5): -110, (7, 1, 6): -110, (7, 1, 7): -80,\n",
    "    (8, 0, 3): -110, (8, 0, 5): -80, (8, 0, 6): -110, (8, 0, 8): -110,\n",
    "    (8, 1, 8): -110,\n",
    "    (9, 0, 3): -110, (9, 0, 6): -80, (9, 0, 7): -110, (9, 0, 9): -110,\n",
    "    (9, 1, 8): -140, (9, 1, 9): -110,\n",
    "    (10, 0, 7): -110, (10, 0, 10): -110, (10, 0, 11): -140,\n",
    "    (10, 1, 8): -140, (10, 1, 9): -140, (10, 1, 10): -110,\n",
    "    (11, 0, 11): -110,\n",
    "    (11, 1, 8): -160, (11, 1, 9): -140, (11, 1, 10): -140, (11, 1, 11): -110\n",
    "}\n",
    "\n",
    "observation_probabilities = {\n",
    "    0: {'o1': 0.9, 'o2': 0.1, 'o3': 0.0},\n",
    "    1: {'o1': 0.8, 'o2': 0.2, 'o3': 0.0},\n",
    "    2: {'o1': 0.7, 'o2': 0.2, 'o3': 0.1},\n",
    "    3: {'o1': 0.1, 'o2': 0.5, 'o3': 0.4},\n",
    "    4: {'o1': 0.9, 'o2': 0.1, 'o3': 0.0},\n",
    "    5: {'o1': 0.8, 'o2': 0.2, 'o3': 0.0},\n",
    "    6: {'o1': 0.7, 'o2': 0.2, 'o3': 0.1},\n",
    "    7: {'o1': 0.1, 'o2': 0.5, 'o3': 0.4},\n",
    "    8: {'o1': 0.9, 'o2': 0.1, 'o3': 0.0},\n",
    "    9: {'o1': 0.8, 'o2': 0.2, 'o3': 0.0},\n",
    "    10: {'o1': 0.7, 'o2': 0.2, 'o3': 0.1},\n",
    "    11: {'o1': 0.1, 'o2': 0.5, 'o3': 0.4}\n",
    "}\n",
    "\n",
    "# Generate Random Beliefs\n",
    "m = 10  # Number of belief points\n",
    "# Number of iterations\n",
    "num_iterations = 10\n",
    "beliefs = np.random.dirichlet(np.ones(num_states), size=m)\n",
    "\n",
    "# Initialize Alpha Vectors\n",
    "alpha_vectors = [np.zeros(num_states) for _ in range(m)]\n",
    "\n",
    "def update_belief(b, action, observation):\n",
    "    \"\"\"Update belief b' given current belief b, action, and observation.\"\"\"\n",
    "    new_belief = np.zeros(num_states)\n",
    "    \n",
    "    for s_prime in range(num_states):\n",
    "        prob_sum = 0\n",
    "        for s in range(num_states):\n",
    "            transitions = transition_probabilities.get(s, {}).get(action, [])\n",
    "            prob_sum += sum(p for sp, p in transitions if sp == s_prime) * b[s]\n",
    "        new_belief[s_prime] = prob_sum * observation_probabilities[s_prime].get(observation, 0)\n",
    "    \n",
    "    new_belief /= np.sum(new_belief)  # Normalize new belief\n",
    "    return new_belief\n",
    "\n",
    "def update_alpha_vector(b, action):\n",
    "    \"\"\"Compute a new alpha vector for a given action.\"\"\"\n",
    "    new_alpha = np.zeros(num_states)\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        # Calculate immediate reward R(s, a, s')\n",
    "        immediate_reward = sum(\n",
    "            prob * reward_action.get((s, action, s_prime), 0)\n",
    "            for s_prime, prob in transition_probabilities.get(s, {}).get(action, [])\n",
    "        )\n",
    "        \n",
    "        # Calculate future reward: sum over states and observations\n",
    "        future_reward = 0\n",
    "        for s_prime, prob in transition_probabilities.get(s, {}).get(action, []):\n",
    "            for o in observations:\n",
    "                obs_prob = observation_probabilities[s_prime].get(o, 0)\n",
    "                b_prime = update_belief(b, action, o)\n",
    "                future_reward += prob * obs_prob * max(\n",
    "                    np.dot(alpha_vectors[i], b_prime) for i in range(len(alpha_vectors))\n",
    "                )\n",
    "\n",
    "        # Update alpha vector with discount factor\n",
    "        new_alpha[s] = immediate_reward + gamma * future_reward\n",
    "    \n",
    "    return new_alpha\n",
    "\n",
    "### Backup Function to Find Best Action\n",
    "def backup(belief):\n",
    "    \"\"\"Backup step to find the best alpha vector and action for a given belief.\"\"\"\n",
    "    best_value = -np.inf\n",
    "    best_alpha = None\n",
    "    best_action = None\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        alpha = update_alpha_vector(belief, a)\n",
    "        value = np.dot(alpha, belief)  # V(b) = α_a^T * b\n",
    "\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_alpha = alpha\n",
    "            best_action = a\n",
    "\n",
    "    return best_alpha, best_action\n",
    "\n",
    "# Plot Alpha Vectors Evolution\n",
    "def plot_alpha_vectors(alpha_vectors, iteration):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, alpha in enumerate(alpha_vectors):\n",
    "        plt.plot(range(len(alpha)), alpha, label=f\"Alpha Vector {i}\")\n",
    "\n",
    "    plt.title(f\"Alpha Vectors after Iteration {iteration}\")\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Alpha Vector Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "for iteration in range(num_iterations):\n",
    "    new_alpha_vectors = []\n",
    "    \n",
    "    # Iterate over belief points\n",
    "    for belief in beliefs:\n",
    "        best_alpha, _ = backup(belief)\n",
    "        new_alpha_vectors.append(best_alpha)\n",
    "\n",
    "    alpha_vectors = new_alpha_vectors  # Update new alpha vectors list\n",
    "    # plot_alpha_vectors(alpha_vectors, iteration + 1)  # Plot the evolution of alpha vectors\n",
    "\n",
    "# Output Optimal Action with Final Alpha Vectors\n",
    "print(\"\\nOptimal Action Based on Each Belief:\")\n",
    "for i, belief in enumerate(beliefs):\n",
    "    print(f\"\\nBelief {i}: {belief}\")\n",
    "\n",
    "    action_values = []\n",
    "    alpha_vectors_action = []\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        alpha = update_alpha_vector(belief, a)\n",
    "        alpha_vectors_action.append(alpha)\n",
    "        value = np.dot(alpha, belief)  # V(b) = α_a^T * b\n",
    "        action_values.append(value)\n",
    "\n",
    "        print(f\"  Alpha vector for action '{actions[a]}': {alpha}\")\n",
    "        print(f\"  Value for action '{actions[a]}': {value}\")\n",
    "\n",
    "    best_action = np.argmax(action_values)\n",
    "    print(f\"  -> Best action: '{actions[best_action]}' with value {action_values[best_action]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c971ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.06113196, 0.20967194, 0.05029107, 0.10603428, 0.01430722,\n",
       "        0.15209329, 0.21423819, 0.04601897, 0.01569096, 0.11352841,\n",
       "        0.01172777, 0.00526594]),\n",
       " [-391.1463125460281, -427.37770283164417],\n",
       " 0,\n",
       " 'continue to manufacture')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply one-step lookahead using the provided belief states, transition, and reward model\n",
    "\n",
    "def compute_one_step_lookahead(belief, action):\n",
    "    \"\"\"Compute the one-step lookahead value for a given belief and action.\"\"\"\n",
    "    # Compute immediate reward R(b, a) = ∑_s b(s) ∑_s' P(s'|s, a) R(s, a, s')\n",
    "    immediate_reward = 0\n",
    "    for s in range(num_states):\n",
    "        belief_s = belief[s]\n",
    "        reward_sum = sum(\n",
    "            prob * reward_action.get((s, action, s_prime), 0)\n",
    "            for s_prime, prob in transition_probabilities.get(s, {}).get(action, [])\n",
    "        )\n",
    "        immediate_reward += belief_s * reward_sum\n",
    "\n",
    "    # Compute future reward ∑_o P(o|b,a) * U(Update(b, a, o))\n",
    "    future_reward = 0\n",
    "    for o in observations:\n",
    "        # P(o|b,a) = ∑_s' P(o|s', a) * ∑_s P(s'|s, a) * b(s)\n",
    "        prob_observation = 0\n",
    "        for s_prime in range(num_states):\n",
    "            obs_prob = observation_probabilities[s_prime].get(o, 0)\n",
    "            transition_prob_sum = sum(\n",
    "                prob * belief[s]\n",
    "                for s in range(num_states)\n",
    "                for sp, prob in transition_probabilities.get(s, {}).get(action, [])\n",
    "                if sp == s_prime\n",
    "            )\n",
    "            prob_observation += obs_prob * transition_prob_sum\n",
    "\n",
    "        # UΓ(Update(b, a, o)) = max(α_i^T * Update(b, a, o))\n",
    "        updated_belief = update_belief(belief, action, o)\n",
    "        max_future_value = max(np.dot(alpha_vector, updated_belief) for alpha_vector in alpha_vectors)\n",
    "        future_reward += prob_observation * max_future_value\n",
    "\n",
    "    # Return the total value for action a\n",
    "    total_value = immediate_reward + gamma * future_reward\n",
    "    return total_value\n",
    "\n",
    "\n",
    "# Evaluate the one-step lookahead for the first belief state in the set for all actions\n",
    "belief_state_index = 0\n",
    "belief = beliefs[belief_state_index]\n",
    "lookahead_values = [compute_one_step_lookahead(belief, a) for a in range(num_actions)]\n",
    "\n",
    "# Determine the best action based on the maximum value\n",
    "best_action_index = np.argmax(lookahead_values)\n",
    "best_action_name = actions[best_action_index]\n",
    "\n",
    "belief, lookahead_values, best_action_index, best_action_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c899e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f121092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53dce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138a102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6500b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0770542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f2908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a32fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
