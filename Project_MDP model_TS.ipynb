{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17084345",
   "metadata": {},
   "source": [
    "# Production and Maintenance Planning (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9e3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows, cols = 3, 4  # Dimensions of the grid (3 rows, 4 columns)\n",
    "num_states = rows * cols  # Total number of states\n",
    "\n",
    "# Convert 2D grid position to 1D state index\n",
    "def state_index(row, col):\n",
    "    return row * cols + col\n",
    "\n",
    "# Convert 1D state index back to 2D grid position (row, col)\n",
    "def state_position(index):\n",
    "    return divmod(index, cols)\n",
    "\n",
    "# Action space\n",
    "actions = ['continue to manufacture', 'shut down and do maintenance']\n",
    "action_index = {'continue to manufacture': 0, 'shut down and do maintenance': 1}\n",
    "\n",
    "\n",
    "# Transition probabilities\n",
    "transition_probabilities = {\n",
    "    0: {\n",
    "        0: [(0, 0.5), (1, 0.4), (2, 0.1)],\n",
    "        1: [(0, 1.0)]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(1, 0.5), (2, 0.4), (3, 0.1)],\n",
    "        1: [(0, 0.9), (1, 0.1)]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(2, 0.5), (3, 0.5)],\n",
    "        1: [(0, 0.6), (1, 0.3), (2, 0.1)]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(3, 1.0)],\n",
    "        1: [(0, 0.3), (1, 0.3), (2, 0.3), (3, 0.1)]\n",
    "    },\n",
    "    4: {\n",
    "        0: [(1, 0.25), (2, 0.25), (3, 0.25), (4, 0.25)],\n",
    "        1: [(4, 1.0)]\n",
    "    },\n",
    "    5: {\n",
    "        0: [(2, 0.6), (3, 0.2), (5, 0.2)],\n",
    "        1: [(4, 0.9), (5, 0.1)]\n",
    "    },\n",
    "    6: {\n",
    "        0: [(3, 0.6), (6, 0.2), (7, 0.2)],\n",
    "        1: [(4, 0.6), (5, 0.3), (6, 0.1)]\n",
    "    },\n",
    "    7: {\n",
    "        0: [(3, 0.3), (7, 0.7)],\n",
    "        1: [(4, 0.3), (5, 0.3), (6, 0.3), (7, 0.1)]\n",
    "    },\n",
    "    8: {\n",
    "        0: [(3, 0.25), (5, 0.25), (6, 0.25), (8, 0.25)],\n",
    "        1: [(8, 1.0)]\n",
    "    },\n",
    "    9: {\n",
    "        0: [(3, 0.25), (6, 0.25), (7, 0.25), (9, 0.25)],\n",
    "        1: [(8, 0.9), (9, 0.1)]\n",
    "    },\n",
    "    10: {\n",
    "        0: [(7, 0.4), (10, 0.3), (11, 0.3)],\n",
    "        1: [(8, 0.6), (9, 0.3), (10, 0.1)]\n",
    "    },\n",
    "    11: {\n",
    "        0: [(11, 1.0)],\n",
    "        1: [(8, 0.3), (9, 0.3), (10, 0.3), (9, 0.1)]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Rewards for each action and state\n",
    "reward_action = {\n",
    "    0: {\n",
    "        0: [-40, -45, -50],\n",
    "        1: [-40, -45, -50],\n",
    "        2: [-40, -45],\n",
    "        3: [-40],\n",
    "        4: [-80, -85, -90, -90],\n",
    "        5: [-80, -85, -90],\n",
    "        6: [-90, -90, -110],\n",
    "        7: [-90, -90],\n",
    "        8: [-110, -80, -110, -110],\n",
    "        9: [-110, -80, -110, -110],\n",
    "        10: [-110, -110, -140],\n",
    "        11: [-110]\n",
    "    },\n",
    "    1: {\n",
    "        0: [-40],\n",
    "        1: [-70, -40],\n",
    "        2: [-70, -70, -40],\n",
    "        3: [-90, -70, -70, -40],\n",
    "        4: [-80],\n",
    "        5: [-110, -80],\n",
    "        6: [-110, -110, -80],\n",
    "        7: [-130, -110, -110, -80],\n",
    "        8: [-110],\n",
    "        9: [-140, -110],\n",
    "        10: [-140, -140, -110],\n",
    "        11: [-160, -140, -140, -110]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bca3d",
   "metadata": {},
   "source": [
    "# Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7cdcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 38 iterations.\n",
      "Optimal State Values:\n",
      "[[-133.33316009 -133.33316009 -133.33316009 -133.33316009]\n",
      " [-181.81800857 -179.84478799 -207.43256729 -231.37237577]\n",
      " [-243.7658039  -254.69589828 -313.86297506 -348.41684312]]\n",
      "\n",
      "Optimal Policy (Value Iteration):\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "gamma = 0.7  # Discount factor\n",
    "theta = 1e-4  # Convergence threshold\n",
    "\n",
    "# Initialize state values\n",
    "state_values = np.zeros(num_states)  # V(s) for all states\n",
    "policy = np.zeros(num_states, dtype=int)  # Policy Ï€(s)\n",
    "# Value iteration with a maximum iteration limit\n",
    "def value_iteration(transition_probabilities, reward_action, gamma, theta, max_iterations=1000):\n",
    "    global state_values, policy\n",
    "    iteration = 0  # Initialize iteration counter\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        delta = 0\n",
    "        new_state_values = state_values.copy()\n",
    "        \n",
    "        # Iterate over all states\n",
    "        for state in range(num_states):\n",
    "            action_values = []\n",
    "            \n",
    "            # Iterate over all actions\n",
    "            for action in range(len(actions)):\n",
    "                value = 0\n",
    "                \n",
    "                # Sum over possible next states\n",
    "                for next_state, prob in transition_probabilities[state][action]:\n",
    "                    reward = reward_action[action].get(state, [0])[0]  # Default reward if not defined\n",
    "                    value += prob * (reward + gamma * state_values[next_state])\n",
    "                \n",
    "                action_values.append(value)\n",
    "            \n",
    "            # Update value of the state\n",
    "            new_state_values[state] = max(action_values)\n",
    "            delta = max(delta, abs(new_state_values[state] - state_values[state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        iteration += 1  # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Converged in {iteration} iterations.\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(f\"Stopped after reaching the maximum of {max_iterations} iterations.\")\n",
    "    \n",
    "    # Extract policy\n",
    "    for state in range(num_states):\n",
    "        action_values = []\n",
    "        for action in range(len(actions)):\n",
    "            value = 0\n",
    "            for next_state, prob in transition_probabilities[state][action]:\n",
    "                reward = reward_action[action].get(state, [0])[0]\n",
    "                value += prob * (reward + gamma * state_values[next_state])\n",
    "            action_values.append(value)\n",
    "        policy[state] = np.argmax(action_values)\n",
    "\n",
    "    return state_values, policy\n",
    "\n",
    "# Run value iteration with a maximum iteration limit\n",
    "state_values, policy = value_iteration(\n",
    "    transition_probabilities, reward_action, gamma, theta, max_iterations=500\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal State Values:\")\n",
    "print(state_values.reshape(rows, cols))\n",
    "print(\"\\nOptimal Policy (Value Iteration):\")\n",
    "print(policy.reshape(rows, cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c534d",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82236df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal State Values:\n",
      "[[-133.33316009 -133.33316009 -133.33316009 -133.33316009]\n",
      " [-181.81800857 -179.84478799 -207.43256729 -231.37237577]\n",
      " [-243.7658039  -254.69589828 -313.86297506 -348.41684312]]\n",
      "\n",
      "Optimal Policy (Policy Iteration):\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]]\n",
      "Policy iteration completed in 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Policy iteration with iteration limit\n",
    "def policy_iteration(transition_probabilities, reward_action, gamma, theta, max_iterations=1000):\n",
    "    global state_values, policy\n",
    "    is_policy_stable = False\n",
    "    iteration = 0  # Track the number of iterations\n",
    "    \n",
    "    while not is_policy_stable and iteration < max_iterations:\n",
    "        # Policy evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_state_values = state_values.copy()\n",
    "            \n",
    "            for state in range(num_states):\n",
    "                action = policy[state]\n",
    "                value = 0\n",
    "                for next_state, prob in transition_probabilities[state][action]:\n",
    "                    reward = reward_action[action].get(state, [0])[0]\n",
    "                    value += prob * (reward + gamma * state_values[next_state])\n",
    "                new_state_values[state] = value\n",
    "                delta = max(delta, abs(new_state_values[state] - state_values[state]))\n",
    "            \n",
    "            state_values = new_state_values\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        # Policy improvement\n",
    "        is_policy_stable = True\n",
    "        for state in range(num_states):\n",
    "            old_action = policy[state]\n",
    "            action_values = []\n",
    "            \n",
    "            for action in range(len(actions)):\n",
    "                value = 0\n",
    "                for next_state, prob in transition_probabilities[state][action]:\n",
    "                    reward = reward_action[action].get(state, [0])[0]\n",
    "                    value += prob * (reward + gamma * state_values[next_state])\n",
    "                action_values.append(value)\n",
    "            \n",
    "            best_action = np.argmax(action_values)  # Find the action with the minimum value\n",
    "            policy[state] = best_action\n",
    "            \n",
    "            if old_action != best_action:\n",
    "                is_policy_stable = False\n",
    "        \n",
    "        iteration += 1  # Increment iteration counter\n",
    "    \n",
    "    return state_values, policy, iteration\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal State Values:\")\n",
    "print(state_values.reshape(rows, cols))\n",
    "print(\"\\nOptimal Policy (Policy Iteration):\")\n",
    "print(policy.reshape(rows, cols))\n",
    "\n",
    "# Run policy iteration with a limit on iterations\n",
    "state_values = np.zeros(num_states)  # Reset state values\n",
    "policy = np.zeros(num_states, dtype=int)  # Reset policy\n",
    "\n",
    "state_values, policy, iterations_used = policy_iteration(transition_probabilities, reward_action, gamma, theta, max_iterations=500)\n",
    "\n",
    "# Print the number of iterations used\n",
    "print(f\"Policy iteration completed in {iterations_used} iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31653554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6597dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef4d2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98863aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e5391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d618856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958021ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052ab2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
